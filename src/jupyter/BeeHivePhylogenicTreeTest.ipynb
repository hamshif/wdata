{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import split, when, lit, row_number, udf, col\n",
    "\n",
    "import random\n",
    "\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"BeeHive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['remove', 'Bee ID', 'remove1', 'DaughtersEfficiencyScore', 'remove2', 'Father SIZE', 'Father TYPE', 'remove3', 'X', 'Y', 'Z']\n",
    "doubles = ['DaughtersEfficiencyScore', 'X', 'Y', 'Z']\n",
    "integers = ['Father SIZE']\n",
    "\n",
    "def struct_field(header, doubles, integers):\n",
    "    \n",
    "    if header in doubles:\n",
    "        return StructField(header, DoubleType())\n",
    "    \n",
    "    if header in integers:\n",
    "        return StructField(header, IntegerType())\n",
    "    \n",
    "    return StructField(header, StringType())\n",
    "\n",
    "fields = [struct_field(header, doubles, integers) for header in headers]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ff885",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).csv('/Users/daniel/dev/duds/pep-data/src/jupyter/dev/test.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f950c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [x for x in df.columns if 'remove' not in x]\n",
    "df = df.select(*cols_to_keep)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df\n",
    "df_cleaned = df_cleaned.withColumn('Cycle', split(df_cleaned['Bee ID'], '_').getItem(0))\\\n",
    "                .withColumn('Cycle ID', split(df_cleaned['Bee ID'], '_').getItem(1))\n",
    "\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycles = sorted([i[0] for i in df_cleaned.select('Cycle').distinct().collect()])\n",
    "# print(cycles)\n",
    "#\n",
    "# def father_cycle(cycle):\n",
    "#     cycle_index = cycles.index(cycle)\n",
    "#     n=3\n",
    "#     father_cyc = None\n",
    "#\n",
    "#     if cycle_index == 0:\n",
    "#         return father_cyc\n",
    "#\n",
    "#     if cycle_index > n:\n",
    "#         father_cyc = random.randint(cycle_index-n, cycle_index-1)\n",
    "#\n",
    "#     elif cycle_index <= n:\n",
    "#         father_cyc = random.randint(0, cycle_index-1)\n",
    "#\n",
    "#     return cycles[father_cyc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddee4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf, col\n",
    "#\n",
    "# convertUDF = udf(lambda z: father_cycle(z))\n",
    "#\n",
    "# df_cleaned.withColumn(\"ParentCycle\", convertUDF(col('Cycle'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9598022",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy('Cycle')\n",
    "df_cleaned = df_cleaned.withColumn('Continuous ID', row_number().over(w))\n",
    "\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b390d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_min_id_per_cycle = {key : value for key, value  in df_cleaned.groupBy('Cycle').min('Continuous ID').collect()}\n",
    "\n",
    "continuous_min_id_per_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5502f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = sorted([i[0] for i in df_cleaned.select('Cycle').distinct().collect()])\n",
    "\n",
    "cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_parent_bee_id(cycle):\n",
    "    n = 3\n",
    "    cycle_index = cycles.index(cycle)\n",
    "\n",
    "    if  not cycle_index :\n",
    "        return None\n",
    "\n",
    "    min_cycle_index = 0\n",
    "\n",
    "    if cycle_index > n:\n",
    "        min_cycle_index = cycle_index - n\n",
    "\n",
    "    parent_bee_id = random.randint(continuous_min_id_per_cycle[cycles[min_cycle_index]], continuous_min_id_per_cycle[cycles[cycle_index]] - 1)\n",
    "\n",
    "    return parent_bee_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23946054",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertUDF = udf(lambda z: assert_parent_bee_id(z))\n",
    "df_cleaned = df_cleaned.withColumn(\"Parent Temp\", convertUDF(col('Cycle')))\\\n",
    "\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb57f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from operator import add\n",
    "def test2(a,b):\n",
    "    return a+b\n",
    "\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).fold(1, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14bf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(tree_dict, row):\n",
    "\n",
    "    # for r in row:\n",
    "    #     if r[10] == 1:\n",
    "    #         tree_dict.append(row)\n",
    "\n",
    "    # for column in tree_dict:\n",
    "    #     if column['Continuous ID'] == row[0]:\n",
    "    #         tree_dict.append(row)\n",
    "\n",
    "    # for column in tree_dict:\n",
    "    #     #tree_dict[column].append(row)\n",
    "    #     for r in list(row):\n",
    "    #         print(r)\n",
    "    #     print(\"-------\")\n",
    "\n",
    "    # for column in tree_dict:\n",
    "    #     print(type(column))\n",
    "    #     # if column['Continuous ID'] == list(row.keys())[0][10]:\n",
    "    #     #     print(row)\n",
    "    # print('---------')\n",
    "\n",
    "    # print(type(row))\n",
    "    # print(row[10])\n",
    "    # #print((list(row))[0][10])\n",
    "\n",
    "    return tree_dict\n",
    "\n",
    "root = df_cleaned.head()\n",
    "df_cleaned.rdd.fold([],test1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59267256",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1:['a'], 2: ['b']}\n",
    "d[1].append('c')\n",
    "print(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
