{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import split, when, lit, row_number, udf, col\n",
    "\n",
    "import random\n",
    "\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"BeeHive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['remove', 'Bee ID', 'remove1', 'DaughtersEfficiencyScore', 'remove2', 'Father SIZE', 'Father TYPE', 'remove3', 'X', 'Y', 'Z']\n",
    "doubles = ['DaughtersEfficiencyScore', 'X', 'Y', 'Z']\n",
    "integers = ['Father SIZE']\n",
    "\n",
    "def struct_field(header, doubles, integers):\n",
    "    \n",
    "    if header in doubles:\n",
    "        return StructField(header, DoubleType())\n",
    "    \n",
    "    if header in integers:\n",
    "        return StructField(header, IntegerType())\n",
    "    \n",
    "    return StructField(header, StringType())\n",
    "\n",
    "fields = [struct_field(header, doubles, integers) for header in headers]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ff885",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).csv('/Users/daniel/dev/duds/pep-data/src/jupyter/BeeHiveTestData.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f950c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [x for x in df.columns if 'remove' not in x]\n",
    "df = df.select(*cols_to_keep)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df\n",
    "df_cleaned = df_cleaned.withColumn('Cycle', split(df_cleaned['Bee ID'], '_').getItem(0))\\\n",
    "                .withColumn('Cycle ID', split(df_cleaned['Bee ID'], '_').getItem(1))\n",
    "\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycles = sorted([i[0] for i in df_cleaned.select('Cycle').distinct().collect()])\n",
    "# print(cycles)\n",
    "#\n",
    "# def father_cycle(cycle):\n",
    "#     cycle_index = cycles.index(cycle)\n",
    "#     n=3\n",
    "#     father_cyc = None\n",
    "#\n",
    "#     if cycle_index == 0:\n",
    "#         return father_cyc\n",
    "#\n",
    "#     if cycle_index > n:\n",
    "#         father_cyc = random.randint(cycle_index-n, cycle_index-1)\n",
    "#\n",
    "#     elif cycle_index <= n:\n",
    "#         father_cyc = random.randint(0, cycle_index-1)\n",
    "#\n",
    "#     return cycles[father_cyc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddee4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf, col\n",
    "#\n",
    "# convertUDF = udf(lambda z: father_cycle(z))\n",
    "#\n",
    "# df_cleaned.withColumn(\"ParentCycle\", convertUDF(col('Cycle'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9598022",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().orderBy('Cycle')\n",
    "df_cleaned = df_cleaned.withColumn('Continuous ID', row_number().over(w))\n",
    "\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b390d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO cycles are not orderd ( order as strings not ints), lower cycles have higher Continuous ID (cycle 2 have higher Continuous ID then 10)\n",
    "continuous_min_id_per_cycle = {key : value for key, value  in df_cleaned.groupBy('Cycle').min('Continuous ID').collect()}\n",
    "\n",
    "continuous_min_id_per_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5502f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add int(i[0]) so cycles will be sorted as integers and not string\n",
    "cycles = sorted([i[0] for i in df_cleaned.select('Cycle').distinct().collect()])\n",
    "\n",
    "cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change func according to changes above\n",
    "def assert_parent_bee_id(cycle):\n",
    "    n = 3\n",
    "    cycle_index = cycles.index(cycle)\n",
    "\n",
    "    if  not cycle_index :\n",
    "        return None\n",
    "\n",
    "    min_cycle_index = 0\n",
    "\n",
    "    if cycle_index > n:\n",
    "        min_cycle_index = cycle_index - n\n",
    "        \n",
    "    start = continuous_min_id_per_cycle[cycles[min_cycle_index]]\n",
    "    end = continuous_min_id_per_cycle[cycles[cycle_index]] - 1\n",
    "    \n",
    "    parent_bee_id = random.randint(start, end)\n",
    "\n",
    "    return parent_bee_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23946054",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertUDF = udf(lambda z: assert_parent_bee_id(z))\n",
    "df_cleaned = df_cleaned\\\n",
    "        .withColumn(\"Parent Temp\", convertUDF(col('Cycle')))\\\n",
    "        .cache()\n",
    "\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from treelib import Node, Tree\n",
    "\n",
    "\n",
    "root = df_cleaned.filter(df_cleaned['Continuous ID'] == 1).collect()[0]\n",
    "\n",
    "working_list = [root]\n",
    "visited_list = []\n",
    "\n",
    "tree = Tree()\n",
    "tree.create_node(root['Bee ID'], root['Bee ID'])\n",
    "\n",
    "while working_list:\n",
    "    parent = working_list.pop()\n",
    "    kids = df_cleaned.filter(df_cleaned['Parent Temp'] == parent['Continuous ID'])\n",
    "\n",
    "    for k in kids.collect():\n",
    "        working_list.append(k)\n",
    "        tree.create_node(k['Bee ID'], k['Bee ID'], parent['Bee ID'] )\n",
    "\n",
    "# tree.create_node(\"Harry\", \"Harry\")  # No parent means its the root node\n",
    "# tree.create_node(\"Jane\",  \"jane\"   , parent=\"Harry\")\n",
    "\n",
    "\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb57f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from operator import add\n",
    "def test2(a,b):\n",
    "    return a+b\n",
    "\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).fold(1, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14bf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(tree_dict, row):\n",
    "\n",
    "    # for r in row:\n",
    "    #     if r[10] == 1:\n",
    "    #         tree_dict.append(row)\n",
    "\n",
    "    # for column in tree_dict:\n",
    "    #     if column['Continuous ID'] == row[0]:\n",
    "    #         tree_dict.append(row)\n",
    "\n",
    "    # for column in tree_dict:\n",
    "    #     #tree_dict[column].append(row)\n",
    "    #     for r in list(row):\n",
    "    #         print(r)\n",
    "    #     print(\"-------\")\n",
    "\n",
    "    # for column in tree_dict:\n",
    "    #     print(type(column))\n",
    "    #     # if column['Continuous ID'] == list(row.keys())[0][10]:\n",
    "    #     #     print(row)\n",
    "    # print('---------')\n",
    "\n",
    "    # print(type(row))\n",
    "    # print(row[10])\n",
    "    # #print((list(row))[0][10])\n",
    "\n",
    "    return tree_dict\n",
    "\n",
    "root = df_cleaned.head()\n",
    "df_cleaned.rdd.fold([],test1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59267256",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1:['a'], 2: ['b']}\n",
    "d[1].append('c')\n",
    "print(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
