{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Bee Hive data https://drive.google.com/file/d/142IBcs6OyQiJxO7owPfkEBFbkrudnh0g/view?usp=sharing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "APP = 'BeeHive'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: parse error near `-m'\r\n",
      "zsh:1: parse error near `-m'\r\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "!{sys.executable} -m pip install -e '../../../Wielder/'\n",
    "!{sys.executable} -m pip install -e '../'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/daniel/hadoop/spark-3.0.3/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/14 12:57:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.10.6 (main, Sep 29 2022 09:14:01)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "from pyspark.sql.functions import split, row_number, udf, col, min, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.shell import sqlContext\n",
    "\n",
    "from pep_data.project import quick_conf\n",
    "from pep_data.spark.util import field_to_struct\n",
    "\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(APP).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyhocon.config_parser:Cannot include file /Users/daniel/dev/duds/pep-data/src/conf/developer.conf. File does not exist or cannot be read.\n"
     ]
    }
   ],
   "source": [
    "# Get app configuration from project.conf file\n",
    "conf = quick_conf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Create schema for the data\n",
    "cols_name = conf[APP]['cols_name']\n",
    "cols_double = conf[APP]['cols_double']\n",
    "cols_integer = conf[APP]['cols_integer']\n",
    "\n",
    "# Create all the fields\n",
    "fields = [field_to_struct(header, doubles=cols_double, integers=cols_integer) for header in cols_name]\n",
    "\n",
    "# Create the schema from th e fields\n",
    "schema = StructType(fields)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Read the data from the csv using the schema\n",
    "data_path = conf[APP]['data_path']\n",
    "df = spark.read.schema(schema).csv(data_path)\n",
    "\n",
    "#df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Remove unnecessary columns(the columns with the word remove in them)\n",
    "cols_to_keep = [x for x in df.columns if 'remove' not in x]\n",
    "df = df.select(*cols_to_keep)\n",
    "\n",
    "#df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Split Bee_ID column to Cycle(Bee value) and Cycle ID(ID value) columns\n",
    "df_cleaned = df.withColumn('Cycle', split(col('Bee ID'), '_')\\\n",
    "                           .getItem(0))\\\n",
    "               .withColumn('Cycle ID', split(col('Bee ID'), '_')\\\n",
    "                           .getItem(1))\n",
    "\n",
    "# Change the type of value in Cycle column from string to integer\n",
    "df_cleaned= df_cleaned.withColumn(\"Cycle\",col(\"Cycle\")\\\n",
    "                                  .cast(IntegerType()))\n",
    "#df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Sort the data frame by Cycle column and add row number for each row(new column with the name Continuous ID)\n",
    "w = Window().orderBy('Cycle')\n",
    "df_cleaned = df_cleaned.withColumn('Continuous ID', row_number()\\\n",
    "                                   .over(w))\n",
    "\n",
    "# df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/14 12:58:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with key = cycle , value = minimum value of Continuous ID of cycle(key)\n",
    "continuous_min_id_per_cycle = {key : value for key, value  in df_cleaned.groupBy('Cycle').min('Continuous ID').collect()}\n",
    "\n",
    "# continuous_min_id_per_cycle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create sorted list of all distinct values of Cycle column\n",
    "cycles = sorted([i[0] for i in df_cleaned.select('Cycle').distinct().collect()])\n",
    "\n",
    "# cycles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Return parent continuous id according to cycle and n\n",
    "def assert_parent_bee_id(cycle):\n",
    "    n = 3\n",
    "\n",
    "    # Get index  of cycle in cycles list\n",
    "    cycle_index = cycles.index(cycle)\n",
    "\n",
    "    # Return parent bee id if cycle is 0\n",
    "    if  not cycle_index :\n",
    "        return None\n",
    "\n",
    "    min_cycle_index = 0\n",
    "\n",
    "    # Update min_cycle_index according to cycle_index and n\n",
    "    if cycle_index > n:\n",
    "        min_cycle_index = cycle_index - n\n",
    "\n",
    "    # Calculate the minimum value for random parent continuous id\n",
    "    min_rand_value = continuous_min_id_per_cycle[cycles[min_cycle_index]]\n",
    "\n",
    "    # Calculate the maximum value for random parent continuous id\n",
    "    max_rand_vale = continuous_min_id_per_cycle[cycles[cycle_index]] - 1\n",
    "\n",
    "    # Get random value of parent_continuous_id (from min_rand_value to max_rand_vale)\n",
    "    parent_continuous_id = random.randint(min_rand_value, max_rand_vale)\n",
    "\n",
    "    return parent_continuous_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/14 12:58:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# TODO group by some columns to create partitions\n",
    "\n",
    "# Convert assert_parent_bee_id(cycle) to user defined function\n",
    "assert_parent_bee_id_udf = udf(lambda z: assert_parent_bee_id(z))\n",
    "\n",
    "# Create new column Parent Continuous ID using the assert_parent_bee_id_udf function and Cycle column\n",
    "# cache() caches the specified data frame in the memory of your cluster's workers\n",
    "# If executing multiple actions on the same data frame then cache it\n",
    "df_cleaned = df_cleaned.withColumn(\"Parent Continuous ID\", assert_parent_bee_id_udf(col('Cycle')))\\\n",
    "                        .cache()\n",
    "\n",
    "# df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Create a DataFrame of parent bees\n",
    "df_parent_bees = df_cleaned.select('Bee ID', 'Continuous ID')\n",
    "df_parent_bees = df_parent_bees.withColumnRenamed(\"Bee ID\",\"Parent Bee ID\")\n",
    "df_parent_bees = df_parent_bees.withColumnRenamed(\"Continuous ID\",\"Temp Continuous ID\")\n",
    "df_parent_bees= df_parent_bees.withColumn(\"Temp Continuous ID\",col(\"Temp Continuous ID\")\\\n",
    "                                  .cast(StringType()))\n",
    "\n",
    "# df_parent_bees.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Add the parent bee id to each bee id with the use of join\n",
    "df_beeId_parent_beeId= df_cleaned.join(df_parent_bees, df_cleaned['Parent Continuous ID'] == df_parent_bees['Temp Continuous ID'],'left').drop('Temp Continuous ID')\n",
    "# df_beeId_parent_beeId.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Split Bee_ID column to Cycle(Bee value) and Cycle ID(ID value) columns\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Parent Cycle', split(col('Parent Bee ID'), '_')\\\n",
    "                           .getItem(0))\n",
    "\n",
    "\n",
    "# Change the type of value in Cycle column from string to integer\n",
    "df_beeId_parent_beeId= df_beeId_parent_beeId.withColumn(\"Parent Cycle\",col(\"Parent Cycle\")\\\n",
    "                                  .cast(IntegerType()))\n",
    "# df_beeId_parent_beeId.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Add oldest Ancestor column\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Oldest Ancestor',\n",
    "                                                         when(col('Cycle')==0,None).\n",
    "                                                         otherwise(''))\n",
    "# df_beeId_parent_beeId.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Add Ancestors column\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Ancestors',\n",
    "                                                         when(col('Cycle')==0,None).\n",
    "                                                         otherwise(''))\n",
    "# df_beeId_parent_beeId.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Assert the values of Ancestors and old Oldest Ancestor colums for the children of cycle 0\n",
    "def assert_ancestors_and_oldestancestors_columns_children(row, list_cycle_0):\n",
    "    row_dict = row.asDict()\n",
    "\n",
    "    for bee in list_cycle_0:\n",
    "        if row_dict['Parent Continuous ID'] == str(bee['Continuous ID']):\n",
    "            row_dict['Oldest Ancestor'] = bee['Bee ID']+'-'+str(bee['Continuous ID'])\n",
    "            row_dict['Ancestors'] = bee['Bee ID']+'-'+str(bee['Continuous ID'])\n",
    "            break\n",
    "\n",
    "    newrow = Row(**row_dict)\n",
    "\n",
    "    return newrow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "      Bee ID  DaughtersEfficiencyScore  Father SIZE Father TYPE          X  \\\n0        0_0                 -0.164749            5      107027  27.961115   \n1        0_9                 -0.091118            8       35473   3.616177   \n2       0_16                 -0.278374            9       72732   6.487132   \n3       0_76                  0.019883            6       49069   8.285177   \n4       0_35                 -0.075842            5         187  -1.000000   \n...      ...                       ...          ...         ...        ...   \n2505  3_2219                 -0.311176           15        1864   8.621328   \n2506  3_1536                 -0.337628           14         174   4.594897   \n2507  3_1073                 -0.201212           10       21859   5.184537   \n2508  3_2072                 -1.019298            5       29237   4.947000   \n2509   3_769                 -0.199861           12       23304   5.896496   \n\n              Y          Z  Cycle Cycle ID  Continuous ID  \\\n0     25.540086  24.798587      0        0              1   \n1     15.399937  14.111507      0        9              2   \n2     27.961115  23.504056      0       16              3   \n3     21.881114  10.805612      0       76              4   \n4     13.000000  17.000000      0       35              5   \n...         ...        ...    ...      ...            ...   \n2505  29.868862  18.359813      3     2219           4403   \n2506  24.424269  25.795047      3     1536           4406   \n2507  17.983514  13.244170      3     1073           4440   \n2508  30.187871  20.174963      3     2072           4442   \n2509  19.862059  12.587156      3      769           4451   \n\n     Parent Continuous ID Parent Bee ID  Parent Cycle Oldest Ancestor  \\\n0                    None          None           NaN            None   \n1                    None          None           NaN            None   \n2                    None          None           NaN            None   \n3                    None          None           NaN            None   \n4                    None          None           NaN            None   \n...                   ...           ...           ...             ...   \n2505                   36          0_23           0.0         0_23-36   \n2506                   80        0_2363           0.0       0_2363-80   \n2507                  232        0_2075           0.0      0_2075-232   \n2508                  238        0_2139           0.0      0_2139-238   \n2509                    7          0_49           0.0          0_49-7   \n\n       Ancestors  \n0           None  \n1           None  \n2           None  \n3           None  \n4           None  \n...          ...  \n2505     0_23-36  \n2506   0_2363-80  \n2507  0_2075-232  \n2508  0_2139-238  \n2509      0_49-7  \n\n[2510 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bee ID</th>\n      <th>DaughtersEfficiencyScore</th>\n      <th>Father SIZE</th>\n      <th>Father TYPE</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Z</th>\n      <th>Cycle</th>\n      <th>Cycle ID</th>\n      <th>Continuous ID</th>\n      <th>Parent Continuous ID</th>\n      <th>Parent Bee ID</th>\n      <th>Parent Cycle</th>\n      <th>Oldest Ancestor</th>\n      <th>Ancestors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0_0</td>\n      <td>-0.164749</td>\n      <td>5</td>\n      <td>107027</td>\n      <td>27.961115</td>\n      <td>25.540086</td>\n      <td>24.798587</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0_9</td>\n      <td>-0.091118</td>\n      <td>8</td>\n      <td>35473</td>\n      <td>3.616177</td>\n      <td>15.399937</td>\n      <td>14.111507</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0_16</td>\n      <td>-0.278374</td>\n      <td>9</td>\n      <td>72732</td>\n      <td>6.487132</td>\n      <td>27.961115</td>\n      <td>23.504056</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0_76</td>\n      <td>0.019883</td>\n      <td>6</td>\n      <td>49069</td>\n      <td>8.285177</td>\n      <td>21.881114</td>\n      <td>10.805612</td>\n      <td>0</td>\n      <td>76</td>\n      <td>4</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0_35</td>\n      <td>-0.075842</td>\n      <td>5</td>\n      <td>187</td>\n      <td>-1.000000</td>\n      <td>13.000000</td>\n      <td>17.000000</td>\n      <td>0</td>\n      <td>35</td>\n      <td>5</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2505</th>\n      <td>3_2219</td>\n      <td>-0.311176</td>\n      <td>15</td>\n      <td>1864</td>\n      <td>8.621328</td>\n      <td>29.868862</td>\n      <td>18.359813</td>\n      <td>3</td>\n      <td>2219</td>\n      <td>4403</td>\n      <td>36</td>\n      <td>0_23</td>\n      <td>0.0</td>\n      <td>0_23-36</td>\n      <td>0_23-36</td>\n    </tr>\n    <tr>\n      <th>2506</th>\n      <td>3_1536</td>\n      <td>-0.337628</td>\n      <td>14</td>\n      <td>174</td>\n      <td>4.594897</td>\n      <td>24.424269</td>\n      <td>25.795047</td>\n      <td>3</td>\n      <td>1536</td>\n      <td>4406</td>\n      <td>80</td>\n      <td>0_2363</td>\n      <td>0.0</td>\n      <td>0_2363-80</td>\n      <td>0_2363-80</td>\n    </tr>\n    <tr>\n      <th>2507</th>\n      <td>3_1073</td>\n      <td>-0.201212</td>\n      <td>10</td>\n      <td>21859</td>\n      <td>5.184537</td>\n      <td>17.983514</td>\n      <td>13.244170</td>\n      <td>3</td>\n      <td>1073</td>\n      <td>4440</td>\n      <td>232</td>\n      <td>0_2075</td>\n      <td>0.0</td>\n      <td>0_2075-232</td>\n      <td>0_2075-232</td>\n    </tr>\n    <tr>\n      <th>2508</th>\n      <td>3_2072</td>\n      <td>-1.019298</td>\n      <td>5</td>\n      <td>29237</td>\n      <td>4.947000</td>\n      <td>30.187871</td>\n      <td>20.174963</td>\n      <td>3</td>\n      <td>2072</td>\n      <td>4442</td>\n      <td>238</td>\n      <td>0_2139</td>\n      <td>0.0</td>\n      <td>0_2139-238</td>\n      <td>0_2139-238</td>\n    </tr>\n    <tr>\n      <th>2509</th>\n      <td>3_769</td>\n      <td>-0.199861</td>\n      <td>12</td>\n      <td>23304</td>\n      <td>5.896496</td>\n      <td>19.862059</td>\n      <td>12.587156</td>\n      <td>3</td>\n      <td>769</td>\n      <td>4451</td>\n      <td>7</td>\n      <td>0_49</td>\n      <td>0.0</td>\n      <td>0_49-7</td>\n      <td>0_49-7</td>\n    </tr>\n  </tbody>\n</table>\n<p>2510 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data frame with additional columns with only cycle 0\n",
    "df_enriched = df_beeId_parent_beeId.filter(col('Cycle')==0)\n",
    "# Data frame with only cycle 0 children\n",
    "df_parent_cycle = df_beeId_parent_beeId.filter(col('Parent Cycle')==0)\n",
    "\n",
    "list_cycle_0 = df_enriched.collect()\n",
    "# Convert the data frame to rdd and assert the values of Oldest Ancestor and Ancestors columns\n",
    "df_parent_cycle_rdd = df_parent_cycle.rdd.map(lambda row: assert_ancestors_and_oldestancestors_columns_children(row,list_cycle_0)).cache()\n",
    "df_parent_cycle = sqlContext.createDataFrame(df_parent_cycle_rdd,schema= df_enriched.schema)\n",
    "# Union the initial enriched data frame with the result\n",
    "df_enriched = df_enriched.union(df_parent_cycle)\n",
    "df_enriched.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Assert the values of Ancestors and old Oldest Ancestor colums for the children of cycle n (all cycles except cycle 0)\n",
    "def assert_ancestors_and_oldestancestors_columns_grandchildren(row, list_n_cycles):\n",
    "\n",
    "    row_dict = row.asDict()\n",
    "\n",
    "    for bee in list_n_cycles:\n",
    "        if row_dict['Parent Continuous ID'] == str(bee['Continuous ID']):\n",
    "            row_dict['Oldest Ancestor'] = bee['Oldest Ancestor']\n",
    "            row_dict['Ancestors'] = bee['Ancestors'] + '->' +row_dict['Parent Bee ID']+'-'+str(row_dict['Parent Continuous ID'])\n",
    "            break\n",
    "\n",
    "    newrow = Row(**row_dict)\n",
    "\n",
    "    return newrow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# For all the cycle except the first, get df of children pf cycle c, get list of its parents\n",
    "# convert df to rdd and use the assert_ancestors_and_oldestancestors_columns_grandchildren to assert\n",
    "# values to the new columns, union the result with the existing enriched df\n",
    "n=3\n",
    "for cycle in cycles[1:-1:]:\n",
    "    df_cycle = df_beeId_parent_beeId.filter(col('Parent Cycle')==cycle)\n",
    "    # list_n_cycles = df_enriched.select('Cycle', 'Continuous ID', 'Oldest Ancestor','Ancestors' )\\\n",
    "    #                             .filter(col('Cycle')==cycle).collect()\n",
    "    list_n_cycles = df_enriched.select('Cycle', 'Continuous ID', 'Oldest Ancestor','Ancestors' )\\\n",
    "                                .filter((col('Parent Cycle')<cycle) & (col('Parent Cycle')>=cycle-n)).collect()\n",
    "\n",
    "    # Convert the data frame to rdd and assert the values of Oldest Ancestor and Ancestors columns\n",
    "    df_cycle_rdd = df_cycle.rdd.map(lambda row: assert_ancestors_and_oldestancestors_columns_grandchildren(row,list_n_cycles)).cache()\n",
    "    df_cycle = sqlContext.createDataFrame(df_cycle_rdd,schema= df_enriched.schema)\n",
    "    # Union the enriched data frame with the result\n",
    "    df_enriched = df_enriched.union(df_cycle)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save data frame to csv file\n",
    "file_name = '2.csv'\n",
    "save_path = conf[APP]['data_save']\n",
    "df_enriched.coalesce(1).write.option('header',True).\\\n",
    "csv(save_path+file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
